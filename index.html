<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <div style="display: flex; align-items: center; margin: 40px 0 20px 0;">
  <img src="static/images/emo.jpg" alt="Logo" style="height: 40px; margin-right: 12px;">
  <h1 style="font-size: 28px; font-weight: bold; margin: 0;">
    EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models
  </h1>
  </div>
  <link rel="icon" type="image/x-icon" href="static/images/emo.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&view_op=list_works&authuser=2&gmla=AOv-ny9R3SoqbB5doThTcuSzrH3RsUiQ4f8yg6e0VNli6py-3cBkfk91uEV5Nz-9FKbYLftROpv96FP3IFF-maunN0U&user=kHYFCd0AAAAJ" target="_blank">He Hu</a><sup>12*</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=nnbFqRAAAAAJ" target="_blank">Yucheng Zhou</a><sup>3*</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=S34nWz0AAAAJ&hl=ja&oi=sra" target="_blank">Lianzhong You</a><sup>2</sup>,</span>
                    <span class="author-block">
                      <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Hongbo Xu</a><sup>2</sup>,</span>
                      <span class="author-block">
                        <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=3SFXtOEAAAAJ" target="_blank">Qianning Wang</a><sup>4</sup>,</span>
              <span class="author-block">
              <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Zheng Lian</a><sup>5</sup>,</span>
                <span class="author-block">
                <a href="https://scholar.google.com/citations?user=zuGMGBoAAAAJ&hl=ja&oi=sra" target="_blank">Fei Richard Yu</a><sup>1</sup>,</span>
                  <span class="author-block">
                  <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Fei Ma</a><sup>2†</sup>,</span> 
                      <span class="author-block">
                      <a href="https://scholar.google.com/citations?user=cb8kYbUAAAAJ&hl=ja&oi=sra" target="_blank">Laizhong Cui</a><sup>12†</sup></span>     
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="eql-cntrb"><small><br><sup>1</sup>College of Computer Science and Software Engineering, Shenzhen University</small></span>
                    <span class="eql-cntrb"><small><br><sup>2</sup>Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)</small></span>
                    <span class="eql-cntrb"><small><br><sup>3</sup>SKL-IOTSC, CIS, University of Macau</small></span>
                    <span class="eql-cntrb"><small><br><sup>4</sup>Auckland University of Technology</small></span>
                    <span class="eql-cntrb"><small><br><sup>5</sup>Institute of Automation, Chinese Academy of Sciences</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2502.04424" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code(Coming Soon)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2502.04424" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
  <!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">What is <b>EmoBench-M</b>?</h2>
      <h2 class="subtitle has-text-justified">
        <span style="font-weight:bold;"><b>EmoBench-M</b>  </span>
        is a multimodal benchmark combining video, audio, and text, targeting basic, conversational, and complex social emotion understanding. Covering 13 real-world scenarios, it fills the gap in dynamic emotion recognition beyond static or single-modality datasets.</h2>
      <img src="static/images/intro_1.png" height="100%"/>
      <!-- <h2 class="subtitle has-text-centered">Example tasks in <span style="font-weight:bold;">BLINK</span>.</h2> -->
      <h2 class="hero-body has-text-centered">
        <br>
        Taxonomy for Evaluating Emotion Intelligence (EI) Capabilities of Multimodal Large Language Models (MLLMs):The diagram outlines the categories of “Foundational Emotion Recognition”, “Conversational Emotion Understanding”, and “Socially Complex Emotion Analysis” along with their respective evaluation scenarios. It also presents a performance comparison of different methods on the proposed dataset EmoBench-M. The “random” baseline refers to a heuristic approach that randomly selects labels from the available candidates.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

  
<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            With the integration of Multimodal large language models (MLLMs) into robotic systems and various AI applications, embedding emotional intelligence (EI) capabilities into these models is essential for enabling robots to effectively address human emotional needs and interact seamlessly in real-world scenarios. While benchmarks for evaluating EI in MLLMs have been developed, they primarily focus on static, text-based, or text-image tasks, overlooking the multimodal complexities of real-world interactions, which often involve video, audio, and dynamic contexts. Based on established psychological theories of EI, we build EmoBench-M, a novel benchmark designed to evaluate the EI capability of MLLMs across 13 valuation scenarios from three key dimensions: foundational emotion recognition, conversational emotion understanding, and socially complex emotion analysis. Evaluations of open-source and closedsource MLLMs on EmoBench-M reveal a significant performance gap between MLLMs and humans across many scenarios. The findings underscore the need for further advancements in EI capabilities of MLLMs. All benchmark resources, including code and datasets, will be publicly released.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/CH-SIMS_page_1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/CH-SIMSv2_page_1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/CMU-MOSEI_page_1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/CMU-MOSI_page_1.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
    <div class="item">
        <!-- Your image here -->
        <img src="static/images/FMSA-SC_page_1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/MC-EIU_page_1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/MELD_page_1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/MER2023_page_1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/MUStARD_page_1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/2.1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/2.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
    <div class="item">
        <!-- Your image here -->
        <img src="static/images/3.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/4.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{hu2025emobench,
  title={EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models},
  author={Hu, He and Zhou, Yucheng and You, Lianzhong and Xu, Hongbo and Wang, Qianning and Lian, Zheng and Yu, Fei Richard and Ma, Fei and Cui, Laizhong},
  journal={arXiv preprint arXiv:2502.04424},
  year={2025}
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
